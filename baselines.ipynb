{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6edb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH       = r\"D:\\University of Illinois Chicago\\Classes\\CS521\\data files\\consolidated_sarcasm_dataset.csv\"\n",
    "GLOVE_EN_PATH  = r\"D:\\University of Illinois Chicago\\Classes\\CS521\\cs521\\glove.6B.300d.txt\"\n",
    "GLOVE_HI_PATH  = r\"D:\\University of Illinois Chicago\\Classes\\CS521\\cs521\\glove-hi.300d.txt\"\n",
    "MBERT = \"bert-base-multilingual-cased\"\n",
    "MBERT_OUT    = \"./baseline_models/bert_mbert\"\n",
    "MODEL_DIR      = \"./baseline_models\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9a660e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, emoji, random, time, joblib, numpy as np, pandas as pd, torch\n",
    "from ftfy import fix_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import re, regex\n",
    "import html, emoji, contractions\n",
    "from ftfy import fix_text\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07c82270",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVANAGARI_TO_ROMAN = True\n",
    "LATIN_BLOCK= r'\\p{Latin}'\n",
    "DEVANAGARI_BLOCK = r'\\p{Devanagari}'\n",
    "MAX_LEN_LSTM   = 60\n",
    "EMBED_DIM      = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898a00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_swear_words = {\n",
    "    # Handles s!@#, s#!t, s**t, etc., variations of \"shit\"\n",
    "    r'\\b[Ss][^A-Za-z\\s]{3,4}\\b': 'shit',\n",
    "    r'[Ss][\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t]+': 'shit',\n",
    "    r'[Ss][#!@$%^&*+\\-=\\[\\]{};:\\'\",.<>?/\\\\|_~`]{2,}[Tt]\\b': 'shit',\n",
    "\n",
    "\n",
    "    # Handles f*ck, f@ck, f**k, f!ck, F*#k, F***, etc., variations of \"fuck\"\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+': 'fuck',\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[c][k]': 'fuck',                # Handles f*ck\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[k]': 'fuck',                   # Handles f***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*#\\!]{1,}[c]?[kz]?': 'fuck',       # Handles F*#k, F***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\w@#$%^&*]+ing\\b': 'fucking',        # Hnadles f#%^ing\n",
    "\n",
    "    # Handles b*tch, b!tch, b1tch, etc., variations of \"bitch\"\n",
    "    r'b[\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[h]+': 'bitch',\n",
    "\n",
    "    # Handles \"fuck\" with various suffixes (e.g., \"f***'z\" -> \"fucks\")\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+[\\'‚Äô]?[zs]?': 'fucks',\n",
    "\n",
    "    # Handles muthaf*ckin, motherf***ing, etc., variations of \"motherfucking\"\n",
    "    r'm[ou]*th[\\!\\@\\#\\$\\%\\^\\&\\*]*[a][\\!\\@\\#\\$\\%\\^\\&\\*]*f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k][\\!\\@\\#\\$\\%\\^\\&\\*]*[i][\\!\\@\\#\\$\\%\\^\\&\\*]*[n][\\!\\@\\#\\$\\%\\^\\&\\*]*': 'motherfucking',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7349252",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_swear_map = {\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúmadarchod‚Äù ‚Äî‚Äî\n",
    "    # Roman + common leet/censor + Devanagari\n",
    "    r'\\b(m[a@]d[ae]?[r]?[\\W_]*ch[o0]d[a@]?|‡§Æ[‡§æ‡§Ö]*‡§¶‡§∞[‡§ö‡§ö]‡•ã‡§¶|‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶)\\b': 'madarchod',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúbhenchod / behenchod‚Äù ‚Äî‚Äî\n",
    "    r'\\b(b[h]*e+h[e]*n[\\W_]*ch[o0]d[a@]?|‡§≠‡•á‡§Ç‡§ö‡•ã‡§¶|‡§¨‡§π[‡•á‡•á]‡§®[‡§ö‡§ö]‡•ã‡§¶|‡§¨‡§π‡§®‡§ö‡•ã‡§¶)\\b': 'bhenchod',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúchutiya / ch**iya‚Äù ‚Äî‚Äî\n",
    "    r'\\b(c[h]*u+t+i+y*a+|ch[u*]+t+iy?[ae]+|‡§ö‡•Ç‡§§‡§ø‡§Ø‡§æ|‡§ö‡•Å‡§§‡§ø‡§Ø‡§æ|‡§õ‡•Ç‡§§‡§ø‡§Ø‡§æ)\\b': 'chutiya',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúchu***‚Äù shortened form ‚Äî‚Äî\n",
    "    r'\\b(ch[\\W_]*u[\\W_]*t[\\W_]*\\*{2,})\\b': 'chut***',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúgandu / gaandu / gaand‚Äù ‚Äî‚Äî\n",
    "    r'\\b(g[a@]a*n+d[u]?[aei]?|‡§ó‡§æ‡§Ç*‡§°‡•Ç?|‡§ó‡§æ‡§£‡•ç‡§°‡•Ç?)\\b': 'gandu',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúgaand‚Äù stand‚Äëalone  ‚Äî‚Äî\n",
    "    r'\\b(ga+a+nd+|‡§ó‡§æ‡§Ç‡§°|‡§ó‡§æ‡§Ç‡§°|‡§ó‡§æ‡§£‡•ç‡§°)\\b': 'gaand',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúlauda / laudae / laude‚Äù ‚Äî‚Äî \n",
    "    r'\\b(l[a@]u+[d]+[aei]*|‡§≤‡•å‡§°‡§º‡§æ|‡§≤‡§Ç‡§°|‡§≤‡•Å‡§Ç‡§°)\\b': 'lauda',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúlund‚Äù ‚Äî‚Äî\n",
    "    r'\\b(l[u]n+d|‡§≤‡•Å‡§Ç‡§°|‡§≤‡§Ç‡§°)\\b': 'lund',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúrandi / r@ndi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(r[a@]n+d[iy]+|‡§∞‡§Ç‡§°‡•Ä|‡§∞‡§Ç‡§°‡§º‡•Ä)\\b': 'randi',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúharami / haramzada / haramzadi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(h[a@]r+a+m[iy]*|‡§π‡§∞‡§æ‡§Æ‡•Ä)\\b': 'harami',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[a@]|‡§π‡§∞‡§æ‡§Æ‡§ú‡§º‡§æ‡§¶‡§æ|‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡§æ)\\b': 'haramzada',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[iy]|‡§π‡§∞‡§æ‡§Æ‡§ú‡§º‡§æ‡§¶‡•Ä|‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡•Ä)\\b': 'haramzadi',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúkamine / kamina / kaminey‚Äù ‚Äî‚Äî\n",
    "    r'\\b(k[a@]m+i+n[e]?[yie]*|‡§ï‡§Æ‡•Ä‡§®‡§æ|‡§ï‡§Æ‡•Ä‡§®‡•á|‡§ï‡§Æ‡•Ä‡§®‡•Ä)\\b': 'kamine',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúkutta / kutte / kuttiya‚Äù ‚Äî‚Äî\n",
    "    r'\\b(k[u]t+t[a@e]*|‡§ï‡•Å‡§§‡•ç‡§§‡§æ|‡§ï‡•Å‡§§‡•ç‡§§‡•á|‡§ï‡•Å‡§§‡•á)\\b': 'kutta',\n",
    "    r'\\b(k[u]t+t[i]y?a+|‡§ï‡•Å‡§§‡§ø‡§Ø‡§æ|‡§ï‡•Å‡§§‡•ç‡§§‡•Ä)\\b': 'kutiya',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúsaala / saale / sali‚Äù ‚Äî‚Äî\n",
    "    r'\\b(s+a+a*l+a+|‡§∏‡§æ‡§≤‡§æ|‡§∏‡§æ‡§≤‡•á)\\b': 'saala',\n",
    "    r'\\b(s+a+l+i+|‡§∏‡§æ‡§≤‡•Ä)\\b': 'sali',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúbakchod / bakchodi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(b[a@]k+ch[o0]d[iy]*|‡§¨‡§ï‡§ö‡•ã‡§¶|‡§¨‡§ï‡§ö‡•ã‡§¶‡•Ä)\\b' : 'bakchod',\n",
    "\n",
    "    # ‚Äî‚Äî milder insults ‚Äî‚Äî\n",
    "    r'\\b(g[a@]dh[a@]a+|‡§ó‡§ß‡§æ|‡§ó‡§ß‡•á)\\b': 'gadha',\n",
    "\n",
    "    # ‚Äî‚Äî catch‚Äëall ‚Äúf**k‚Äù in Roman Hindi sentences ‚Äî‚Äî\n",
    "    r'\\b(f[\\W_]*u[\\W_]*c[\\W_]*k+)\\b': 'fuck',\n",
    "\n",
    "    # ‚Äî‚Äî emojis or censor blocks containing DevanƒÅgarƒ´ letters and stars\n",
    "    r'[\\u0900-\\u097F]\\*{2,}[\\u0900-\\u097F]*': 'censored_hindi_word',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011ae2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_emoticons = {\n",
    "    ':-)': 'smiley_face',\n",
    "    ':)': 'smiley_face',\n",
    "    ':-D': 'grinning_face',\n",
    "    ':D': 'grinning_face',\n",
    "    ':-(': 'sad_face',\n",
    "    ':(': 'sad_face',\n",
    "    ':-P': 'playful_face',\n",
    "    ':P': 'playful_face',\n",
    "    ':-p': 'playful_face',\n",
    "    ':p': 'playful_face',\n",
    "    ';-)': 'winking_face',\n",
    "    ';)': 'winking_face',\n",
    "    ':-O': 'surprised_face',\n",
    "    ':O': 'surprised_face',\n",
    "    ':-o': 'surprised_face',\n",
    "    ':o': 'surprised_face',\n",
    "    r':-/': 'skeptical_face',\n",
    "    r':/': 'skeptical_face',\n",
    "    r':-\\|': 'neutral_face',\n",
    "    r':\\|': 'neutral_face',\n",
    "    '<3': 'heart',\n",
    "    '^_^': 'happy_face',\n",
    "    '-_-': 'expressionless_face',\n",
    "    'o_O': 'confused_face',\n",
    "    'O_o': 'confused_face',\n",
    "    'o_o': 'confused_face',\n",
    "    '>_<': 'frustrated_face',\n",
    "    'O.o': 'confused_face',\n",
    "    'o.O': 'confused_face',\n",
    "    '0_o': 'confused_face',\n",
    "    'o_0': 'confused_face',\n",
    "    'xD': 'laughing_face',\n",
    "    'XD': 'laughing_face',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf9dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text) if isinstance(text, str) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        # Attempt to encode to 'latin1' and decode to 'utf-8' to fix encoding issues\n",
    "        clean_text = text.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        # Fallback: ignore encoding errors if they occur\n",
    "        clean_text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e79261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_html_entities(text):\n",
    "    \"\"\"Decode HTML entities in text, such as '&amp;' to '&'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Use html.unescape to decode all HTML entities\n",
    "    clean_text = html.unescape(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110e89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = re.sub(r'</?[^>]+>', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5a732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_urls(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match URLs\n",
    "    url_pattern = r'(http[s]?://\\S+|www\\.\\S+)'\n",
    "    clean_text = re.sub(url_pattern, ' SomeWebLink ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc662a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emails(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match email addresses\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "    clean_text = re.sub(email_pattern, ' EMAILADDRESS ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "556254f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_censored_swear_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = text\n",
    "    for pattern, replacement in censored_swear_words.items():\n",
    "        # Do not add word boundaries if pattern already contains them\n",
    "        if pattern.startswith(r'\\b') and pattern.endswith(r'\\b'):\n",
    "            clean_pattern = pattern\n",
    "        else:\n",
    "            clean_pattern = r'\\b' + pattern + r'\\b'\n",
    "        clean_text = re.sub(clean_pattern, replacement, clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b38722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hindi_swears(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    out = text\n",
    "    for pat, repl in hindi_swear_map.items():\n",
    "        out = regex.sub(pat, repl, out, flags=regex.IGNORECASE)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9738bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mentions(text):\n",
    "    \"\"\"Replace mentions with 'SomeTaggedAccount', including when attached to other words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match @ followed by word characters\n",
    "    # Replace mentions with spaces around to prevent word merging\n",
    "    mention_pattern = r'@([A-Za-z0-9_]+)'\n",
    "    clean_text = re.sub(mention_pattern, ' SomeTaggedAccount ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f41aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_at_symbol(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace '@' when used as 'at'\n",
    "    # Conditions:\n",
    "    # - '@' followed by a space\n",
    "    # - '@' at the end of a string\n",
    "    # - '@' followed by a number or time format\n",
    "    # - '@' followed by a capitalized word (assuming location or time)\n",
    "    clean_text = re.sub(r'@\\s', 'at ', text)\n",
    "    clean_text = re.sub(r'@$', 'at', clean_text)\n",
    "    clean_text = re.sub(r'@(?=\\d)', 'at ', clean_text)\n",
    "    clean_text = re.sub(r'@ (?=[A-Z])', 'at ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2451000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove '#' symbol in various contexts\n",
    "    clean_text = re.sub(r'(^|\\s)#(\\w*[^\\s\\w]?)', r'\\1\\2', text)\n",
    "    clean_text = re.sub(r'(\\w+)#([^\\s\\w]?)', r'\\1\\2', clean_text)\n",
    "    clean_text = re.sub(r'([^\\s\\w])#', r'\\1', clean_text)\n",
    "\n",
    "    # Remove any remaining standalone '#' symbols\n",
    "    clean_text = re.sub(r'(?<=\\s)#(?=\\s|$)|(?<=^)#(?=\\s|$)', '', clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d317760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    clean_text = ' '.join(expanded_words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a135c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicode_punct(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation *except* ? ! . , and keep both Latin & Devanagari letters.\n",
    "    Any other symbol is dropped.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # keep letters, numbers, spaces and selected punctuation\n",
    "    pattern = rf'[^\\s{LATIN_BLOCK}{DEVANAGARI_BLOCK}0-9\\?\\!\\.,]'\n",
    "    return regex.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b343ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_devanagari(text: str) -> str:\n",
    "    if not DEVANAGARI_TO_ROMAN or not isinstance(text, str):\n",
    "        return text\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9379c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_colons(text):\n",
    "    \"\"\"Remove colons around emoji descriptions in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match emoji descriptions with colons\n",
    "    pattern = r':([a-zA-Z0-9_+-]+):'\n",
    "    # Replace matches with the emoji description without colons\n",
    "    clean_text = re.sub(pattern, r'\\1', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a35db5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ascii_emoticons(text):\n",
    "    \"\"\"Replace ASCII emoticons in text with their descriptions.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    clean_text = text\n",
    "    for emoticon, description in ascii_emoticons.items():\n",
    "        # Escape the emoticon pattern to handle special characters\n",
    "        escaped_emoticon = re.escape(emoticon)\n",
    "        # Build the regex pattern to match emoticons not part of words\n",
    "        pattern = r'(?<!\\w)' + escaped_emoticon + r'(?!\\w)'\n",
    "        # Replace the emoticon with the description, ignoring case\n",
    "        clean_text = re.sub(pattern, f\"{description}\", clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a45e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trade_mark(text):\n",
    "    \"\"\"Replace 'trade_mark' with a single quote (').\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace 'trade_mark' with \"'\"\n",
    "    clean_text = text.replace(\"trade_mark\", \"'\")\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840e82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = ' '.join(text.split())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ace8324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_functions = [\n",
    "    replace_emojis,\n",
    "    normalize_text,\n",
    "    decode_html_entities,\n",
    "    remove_html_tags,\n",
    "    replace_urls,\n",
    "    replace_emails,\n",
    "    replace_censored_swear_words,\n",
    "    replace_hindi_swears,\n",
    "    replace_mentions,\n",
    "    replace_at_symbol,\n",
    "    remove_hashtags,\n",
    "    expand_contractions,\n",
    "    clean_unicode_punct,\n",
    "    transliterate_devanagari,\n",
    "    remove_emoji_colons,\n",
    "    replace_ascii_emoticons,\n",
    "    replace_trade_mark,\n",
    "    remove_extra_whitespace\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4844e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cleaning(text: str) -> str:\n",
    "    \"\"\"Apply every cleaning function in the exact order given.\"\"\"\n",
    "    for fn in cleaning_functions:\n",
    "        text = fn(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b0a2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH).dropna(subset=[\"comment\", \"label\"])\n",
    "df[\"clean\"] = df[\"comment\"].astype(str).map(apply_cleaning)\n",
    "\n",
    "label_map = {'non_sarcastic': 0, 'sarcastic': 1}\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip().map(label_map)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df[[\"clean\", \"label\"]],\n",
    "    test_size=0.20,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ metrics helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def metrics(y_true, y_pred, name=\"\"):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "    print(f\"{name:12}  Acc {acc:.4f} | Prec {p:.4f} | Rec {r:.4f} | F1 {f:.4f}\")\n",
    "    return acc, p, r, f\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1b4a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Training TF-IDF + LinearSVM baseline ‚Ä¶\n",
      "TFIDF-SVM     Acc 0.7149 | Prec 0.7509 | Rec 0.7849 | F1 0.7675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./baseline_models/tfidf_svm.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nüîß Training TF-IDF + LinearSVM baseline ‚Ä¶\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_df=0.95,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True,\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    ")\n",
    "\n",
    "X_train = tfidf.fit_transform(train_df[\"clean\"])\n",
    "X_test  = tfidf.transform(test_df[\"clean\"])\n",
    "\n",
    "svm = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=42)\n",
    "svm.fit(X_train, train_df[\"label\"])\n",
    "\n",
    "pred_svm = svm.predict(X_test)\n",
    "\n",
    "metrics(test_df[\"label\"], pred_svm, \"TFIDF-SVM\")\n",
    "\n",
    "joblib.dump((tfidf, svm), f\"{MODEL_DIR}/tfidf_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a669a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Training BiLSTM baseline ‚Ä¶ (loading embeddings once)\n",
      "‚Ä¢ Detected header line in D:\\University of Illinois Chicago\\Classes\\CS521\\cs521\\glove-hi.300d.txt, skipping...\n",
      "   ‚Ä¢ Epoch 1/6  last-batch loss 0.6009\n",
      "   ‚Ä¢ Epoch 2/6  last-batch loss 0.4546\n",
      "   ‚Ä¢ Epoch 3/6  last-batch loss 0.2991\n",
      "   ‚Ä¢ Epoch 4/6  last-batch loss 0.2513\n",
      "   ‚Ä¢ Epoch 5/6  last-batch loss 0.0932\n",
      "   ‚Ä¢ Epoch 6/6  last-batch loss 0.0649\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîß Training BiLSTM baseline ‚Ä¶ (loading embeddings once)\")\n",
    "\n",
    "def load_glove(path):\n",
    "    \"\"\"\n",
    "    Load GloVe or FastText embeddings into a dict.\n",
    "    If the first line is a header (e.g., '351320 300'), skip it automatically.\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline()\n",
    "        # If first line is two integers, assume it's a header (FastText format)\n",
    "        if len(first_line.split()) == 2 and all(x.isdigit() for x in first_line.split()):\n",
    "            print(f\"‚Ä¢ Detected header line in {path}, skipping...\")\n",
    "        else:\n",
    "            # No header; reset file read pointer\n",
    "            f.seek(0)\n",
    "        \n",
    "        for line in f:\n",
    "            parts = line.rstrip().split()\n",
    "            if len(parts) < 10:  # sanity: ignore malformed lines\n",
    "                continue\n",
    "            word, vec = parts[0], np.asarray(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vec\n",
    "    return glove\n",
    "\n",
    "glove_en = load_glove(GLOVE_EN_PATH)\n",
    "glove_hi = load_glove(GLOVE_HI_PATH)\n",
    "\n",
    "token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "vectors   = [np.zeros(EMBED_DIM), np.random.randn(EMBED_DIM) * 0.05]\n",
    "\n",
    "def add_word(w):\n",
    "    if w not in token2idx:\n",
    "        token2idx[w] = len(token2idx)\n",
    "        if w in glove_en: vectors.append(glove_en[w])\n",
    "        elif w in glove_hi: vectors.append(glove_hi[w])\n",
    "        else: vectors.append(np.random.randn(EMBED_DIM) * 0.05)\n",
    "\n",
    "for txt in train_df[\"clean\"]:\n",
    "    for tok in txt.split():\n",
    "        add_word(tok)\n",
    "\n",
    "emb_matrix = torch.tensor(np.stack(vectors), dtype=torch.float)\n",
    "\n",
    "class HinglishDS(TorchDataset): \n",
    "    def __init__(self, texts, labels, maxlen=MAX_LEN_LSTM):\n",
    "        self.texts, self.labels, self.maxlen = texts, labels, maxlen\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        toks  = self.texts.iloc[idx].split()              \n",
    "        idxs  = [token2idx.get(t, 1) for t in toks][:self.maxlen]\n",
    "        pad   = [0] * (self.maxlen - len(idxs))\n",
    "        label = self.labels.iloc[idx]                     \n",
    "        return torch.tensor(idxs + pad), torch.tensor(label)\n",
    "\n",
    "\n",
    "train_dl = DataLoader(HinglishDS(train_df[\"clean\"], train_df[\"label\"]),\n",
    "                      batch_size=64, shuffle=True)\n",
    "test_dl  = DataLoader(HinglishDS(test_df[\"clean\"],  test_df[\"label\"]),\n",
    "                      batch_size=128)\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, emb, hidden=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(emb, freeze=False, padding_idx=0)\n",
    "        self.lstm  = nn.LSTM(emb.size(1), hidden, bidirectional=True, batch_first=True)\n",
    "        self.fc    = nn.Linear(hidden * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)\n",
    "        _, (h, _) = self.lstm(emb)\n",
    "        h = torch.cat([h[0], h[1]], dim=1)\n",
    "        return self.fc(h)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = BiLSTM(emb_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 7):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(f\"   ‚Ä¢ Epoch {epoch}/6  last-batch loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "359fc53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM        Acc 0.7149 | Prec 0.8145 | Rec 0.6792 | F1 0.7407\n",
      "\n",
      "  Done ‚Äì models saved in ./baseline_models/\n"
     ]
    }
   ],
   "source": [
    "model.eval(); preds, truth = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        preds.extend(torch.argmax(model(xb.to(device)), 1).cpu().tolist())\n",
    "        truth.extend(yb.tolist())\n",
    "\n",
    "metrics(truth, preds, \"BiLSTM\")\n",
    "\n",
    "torch.save(model.state_dict(),       f\"{MODEL_DIR}/bilstm_state.pt\")\n",
    "np.save(f\"{MODEL_DIR}/vocab.npy\",    np.array(list(token2idx.items()), dtype=object))\n",
    "\n",
    "print(\"\\n  Done ‚Äì models saved in ./baseline_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ede09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH).dropna(subset=[\"comment\", \"label\"])\n",
    "label_map = {\"non_sarcastic\": 0, \"sarcastic\": 1}\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip().map(label_map)\n",
    "df[\"clean\"] = df[\"comment\"].astype(str).map(apply_cleaning)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df[[\"clean\", \"label\"]], test_size=0.15, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "train_ds = HFDataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds   = HFDataset.from_pandas(val_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b19db224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [00:00<00:00, 18127.28 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 332/332 [00:00<00:00, 18420.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MBERT)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"clean\"], padding=\"max_length\",\n",
    "                     truncation=True, max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"clean\"])\n",
    "val_ds   = val_ds.map(tokenize,   batched=True, remove_columns=[\"clean\"])\n",
    "train_ds.set_format(\"torch\")\n",
    "val_ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cac9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MBERT, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f5c6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc   = accuracy_score(labels, preds)\n",
    "    p,r,f,_ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cf8d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manan\\anaconda3\\envs\\cs521\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_11288\\3324455076.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir          = MBERT_OUT,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy       = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model  = \"f1\",\n",
    "    learning_rate       = 2e-5,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size  = BATCH_SIZE,\n",
    "    num_train_epochs    = EPOCHS,\n",
    "    weight_decay        = 0.01,\n",
    "    fp16                = torch.cuda.is_available(),   # mixed precision if GPU\n",
    "    report_to           = \"none\",\n",
    "    logging_steps       = 25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = val_ds,\n",
    "    tokenizer       = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47d1f842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='472' max='472' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [472/472 01:15, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.587437</td>\n",
       "      <td>0.683735</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.613065</td>\n",
       "      <td>0.699140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.497178</td>\n",
       "      <td>0.756024</td>\n",
       "      <td>0.730469</td>\n",
       "      <td>0.939698</td>\n",
       "      <td>0.821978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.498398</td>\n",
       "      <td>0.780120</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.818859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.550245</td>\n",
       "      <td>0.789157</td>\n",
       "      <td>0.789238</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>0.834123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=472, training_loss=0.40685607013055836, metrics={'train_runtime': 76.2869, 'train_samples_per_second': 98.313, 'train_steps_per_second': 6.187, 'total_flos': 493333228800000.0, 'train_loss': 0.40685607013055836, 'epoch': 4.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00561dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./baseline_models/bert_mbert\\\\tokenizer_config.json',\n",
       " './baseline_models/bert_mbert\\\\special_tokens_map.json',\n",
       " './baseline_models/bert_mbert\\\\vocab.txt',\n",
       " './baseline_models/bert_mbert\\\\added_tokens.json',\n",
       " './baseline_models/bert_mbert\\\\tokenizer.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(MBERT_OUT, exist_ok=True)\n",
    "trainer.save_model(MBERT_OUT)\n",
    "tokenizer.save_pretrained(MBERT_OUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
