{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d75280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, joblib, torch, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import re, regex\n",
    "import html, emoji, contractions\n",
    "from ftfy import fix_text\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f45250",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT          = r\"D:\\University of Illinois Chicago\\Classes\\CS521\\cs521\"\n",
    "BATCH_DIR     = os.path.join(ROOT, \"labeled_batches\")\n",
    "MODEL_DIR     = os.path.join(ROOT, \"baseline_models\")\n",
    "HINGBERT_DIR  = os.path.join(ROOT, \"sarcasm_hingbert_model\")\n",
    "DEVANAGARI_TO_ROMAN = True\n",
    "LATIN_BLOCK= r'\\p{Latin}'\n",
    "DEVANAGARI_BLOCK = r'\\p{Devanagari}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90cd9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_swear_words = {\n",
    "    # Handles s!@#, s#!t, s**t, etc., variations of \"shit\"\n",
    "    r'\\b[Ss][^A-Za-z\\s]{3,4}\\b': 'shit',\n",
    "    r'[Ss][\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t]+': 'shit',\n",
    "    r'[Ss][#!@$%^&*+\\-=\\[\\]{};:\\'\",.<>?/\\\\|_~`]{2,}[Tt]\\b': 'shit',\n",
    "\n",
    "\n",
    "    # Handles f*ck, f@ck, f**k, f!ck, F*#k, F***, etc., variations of \"fuck\"\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+': 'fuck',\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[c][k]': 'fuck',                # Handles f*ck\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[k]': 'fuck',                   # Handles f***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*#\\!]{1,}[c]?[kz]?': 'fuck',       # Handles F*#k, F***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\w@#$%^&*]+ing\\b': 'fucking',        # Hnadles f#%^ing\n",
    "\n",
    "    # Handles b*tch, b!tch, b1tch, etc., variations of \"bitch\"\n",
    "    r'b[\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[h]+': 'bitch',\n",
    "\n",
    "    # Handles \"fuck\" with various suffixes (e.g., \"f***'z\" -> \"fucks\")\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+[\\'’]?[zs]?': 'fucks',\n",
    "\n",
    "    # Handles muthaf*ckin, motherf***ing, etc., variations of \"motherfucking\"\n",
    "    r'm[ou]*th[\\!\\@\\#\\$\\%\\^\\&\\*]*[a][\\!\\@\\#\\$\\%\\^\\&\\*]*f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k][\\!\\@\\#\\$\\%\\^\\&\\*]*[i][\\!\\@\\#\\$\\%\\^\\&\\*]*[n][\\!\\@\\#\\$\\%\\^\\&\\*]*': 'motherfucking',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b8c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_swear_map = {\n",
    "\n",
    "    # —— “madarchod” ——\n",
    "    # Roman + common leet/censor + Devanagari\n",
    "    r'\\b(m[a@]d[ae]?[r]?[\\W_]*ch[o0]d[a@]?|म[ाअ]*दर[चच]ोद|मादरचोद)\\b': 'madarchod',\n",
    "\n",
    "    # —— “bhenchod / behenchod” ——\n",
    "    r'\\b(b[h]*e+h[e]*n[\\W_]*ch[o0]d[a@]?|भेंचोद|बह[ेे]न[चच]ोद|बहनचोद)\\b': 'bhenchod',\n",
    "\n",
    "    # —— “chutiya / ch**iya” ——\n",
    "    r'\\b(c[h]*u+t+i+y*a+|ch[u*]+t+iy?[ae]+|चूतिया|चुतिया|छूतिया)\\b': 'chutiya',\n",
    "\n",
    "    # —— “chu***” shortened form ——\n",
    "    r'\\b(ch[\\W_]*u[\\W_]*t[\\W_]*\\*{2,})\\b': 'chut***',\n",
    "\n",
    "    # —— “gandu / gaandu / gaand” ——\n",
    "    r'\\b(g[a@]a*n+d[u]?[aei]?|गां*डू?|गाण्डू?)\\b': 'gandu',\n",
    "\n",
    "    # —— “gaand” stand‑alone  ——\n",
    "    r'\\b(ga+a+nd+|गांड|गांड|गाण्ड)\\b': 'gaand',\n",
    "\n",
    "    # —— “lauda / laudae / laude” —— \n",
    "    r'\\b(l[a@]u+[d]+[aei]*|लौड़ा|लंड|लुंड)\\b': 'lauda',\n",
    "\n",
    "    # —— “lund” ——\n",
    "    r'\\b(l[u]n+d|लुंड|लंड)\\b': 'lund',\n",
    "\n",
    "    # —— “randi / r@ndi” ——\n",
    "    r'\\b(r[a@]n+d[iy]+|रंडी|रंड़ी)\\b': 'randi',\n",
    "\n",
    "    # —— “harami / haramzada / haramzadi” ——\n",
    "    r'\\b(h[a@]r+a+m[iy]*|हरामी)\\b': 'harami',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[a@]|हरामज़ादा|हरामजादा)\\b': 'haramzada',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[iy]|हरामज़ादी|हरामजादी)\\b': 'haramzadi',\n",
    "\n",
    "    # —— “kamine / kamina / kaminey” ——\n",
    "    r'\\b(k[a@]m+i+n[e]?[yie]*|कमीना|कमीने|कमीनी)\\b': 'kamine',\n",
    "\n",
    "    # —— “kutta / kutte / kuttiya” ——\n",
    "    r'\\b(k[u]t+t[a@e]*|कुत्ता|कुत्ते|कुते)\\b': 'kutta',\n",
    "    r'\\b(k[u]t+t[i]y?a+|कुतिया|कुत्ती)\\b': 'kutiya',\n",
    "\n",
    "    # —— “saala / saale / sali” ——\n",
    "    r'\\b(s+a+a*l+a+|साला|साले)\\b': 'saala',\n",
    "    r'\\b(s+a+l+i+|साली)\\b': 'sali',\n",
    "\n",
    "    # —— “bakchod / bakchodi” ——\n",
    "    r'\\b(b[a@]k+ch[o0]d[iy]*|बकचोद|बकचोदी)\\b' : 'bakchod',\n",
    "\n",
    "    # —— milder insults ——\n",
    "    r'\\b(g[a@]dh[a@]a+|गधा|गधे)\\b': 'gadha',\n",
    "\n",
    "    # —— catch‑all “f**k” in Roman Hindi sentences ——\n",
    "    r'\\b(f[\\W_]*u[\\W_]*c[\\W_]*k+)\\b': 'fuck',\n",
    "\n",
    "    # —— emojis or censor blocks containing Devanāgarī letters and stars\n",
    "    r'[\\u0900-\\u097F]\\*{2,}[\\u0900-\\u097F]*': 'censored_hindi_word',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4757fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_emoticons = {\n",
    "    ':-)': 'smiley_face',\n",
    "    ':)': 'smiley_face',\n",
    "    ':-D': 'grinning_face',\n",
    "    ':D': 'grinning_face',\n",
    "    ':-(': 'sad_face',\n",
    "    ':(': 'sad_face',\n",
    "    ':-P': 'playful_face',\n",
    "    ':P': 'playful_face',\n",
    "    ':-p': 'playful_face',\n",
    "    ':p': 'playful_face',\n",
    "    ';-)': 'winking_face',\n",
    "    ';)': 'winking_face',\n",
    "    ':-O': 'surprised_face',\n",
    "    ':O': 'surprised_face',\n",
    "    ':-o': 'surprised_face',\n",
    "    ':o': 'surprised_face',\n",
    "    r':-/': 'skeptical_face',\n",
    "    r':/': 'skeptical_face',\n",
    "    r':-\\|': 'neutral_face',\n",
    "    r':\\|': 'neutral_face',\n",
    "    '<3': 'heart',\n",
    "    '^_^': 'happy_face',\n",
    "    '-_-': 'expressionless_face',\n",
    "    'o_O': 'confused_face',\n",
    "    'O_o': 'confused_face',\n",
    "    'o_o': 'confused_face',\n",
    "    '>_<': 'frustrated_face',\n",
    "    'O.o': 'confused_face',\n",
    "    'o.O': 'confused_face',\n",
    "    '0_o': 'confused_face',\n",
    "    'o_0': 'confused_face',\n",
    "    'xD': 'laughing_face',\n",
    "    'XD': 'laughing_face',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57ee615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text) if isinstance(text, str) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d9af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        # Attempt to encode to 'latin1' and decode to 'utf-8' to fix encoding issues\n",
    "        clean_text = text.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        # Fallback: ignore encoding errors if they occur\n",
    "        clean_text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c456b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_html_entities(text):\n",
    "    \"\"\"Decode HTML entities in text, such as '&amp;' to '&'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Use html.unescape to decode all HTML entities\n",
    "    clean_text = html.unescape(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda703e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = re.sub(r'</?[^>]+>', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1899c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_urls(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match URLs\n",
    "    url_pattern = r'(http[s]?://\\S+|www\\.\\S+)'\n",
    "    clean_text = re.sub(url_pattern, ' SomeWebLink ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab47e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emails(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match email addresses\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "    clean_text = re.sub(email_pattern, ' EMAILADDRESS ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd48787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_censored_swear_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = text\n",
    "    for pattern, replacement in censored_swear_words.items():\n",
    "        # Do not add word boundaries if pattern already contains them\n",
    "        if pattern.startswith(r'\\b') and pattern.endswith(r'\\b'):\n",
    "            clean_pattern = pattern\n",
    "        else:\n",
    "            clean_pattern = r'\\b' + pattern + r'\\b'\n",
    "        clean_text = re.sub(clean_pattern, replacement, clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f04d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hindi_swears(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    out = text\n",
    "    for pat, repl in hindi_swear_map.items():\n",
    "        out = regex.sub(pat, repl, out, flags=regex.IGNORECASE)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfac78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mentions(text):\n",
    "    \"\"\"Replace mentions with 'SomeTaggedAccount', including when attached to other words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match @ followed by word characters\n",
    "    # Replace mentions with spaces around to prevent word merging\n",
    "    mention_pattern = r'@([A-Za-z0-9_]+)'\n",
    "    clean_text = re.sub(mention_pattern, ' SomeTaggedAccount ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916b996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_at_symbol(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace '@' when used as 'at'\n",
    "    # Conditions:\n",
    "    # - '@' followed by a space\n",
    "    # - '@' at the end of a string\n",
    "    # - '@' followed by a number or time format\n",
    "    # - '@' followed by a capitalized word (assuming location or time)\n",
    "    clean_text = re.sub(r'@\\s', 'at ', text)\n",
    "    clean_text = re.sub(r'@$', 'at', clean_text)\n",
    "    clean_text = re.sub(r'@(?=\\d)', 'at ', clean_text)\n",
    "    clean_text = re.sub(r'@ (?=[A-Z])', 'at ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ce57afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove '#' symbol in various contexts\n",
    "    clean_text = re.sub(r'(^|\\s)#(\\w*[^\\s\\w]?)', r'\\1\\2', text)\n",
    "    clean_text = re.sub(r'(\\w+)#([^\\s\\w]?)', r'\\1\\2', clean_text)\n",
    "    clean_text = re.sub(r'([^\\s\\w])#', r'\\1', clean_text)\n",
    "\n",
    "    # Remove any remaining standalone '#' symbols\n",
    "    clean_text = re.sub(r'(?<=\\s)#(?=\\s|$)|(?<=^)#(?=\\s|$)', '', clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd153a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    clean_text = ' '.join(expanded_words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "681cad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicode_punct(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation *except* ? ! . , and keep both Latin & Devanagari letters.\n",
    "    Any other symbol is dropped.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # keep letters, numbers, spaces and selected punctuation\n",
    "    pattern = rf'[^\\s{LATIN_BLOCK}{DEVANAGARI_BLOCK}0-9\\?\\!\\.,]'\n",
    "    return regex.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7142fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_devanagari(text: str) -> str:\n",
    "    if not DEVANAGARI_TO_ROMAN or not isinstance(text, str):\n",
    "        return text\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e91ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_colons(text):\n",
    "    \"\"\"Remove colons around emoji descriptions in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match emoji descriptions with colons\n",
    "    pattern = r':([a-zA-Z0-9_+-]+):'\n",
    "    # Replace matches with the emoji description without colons\n",
    "    clean_text = re.sub(pattern, r'\\1', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1defc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ascii_emoticons(text):\n",
    "    \"\"\"Replace ASCII emoticons in text with their descriptions.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    clean_text = text\n",
    "    for emoticon, description in ascii_emoticons.items():\n",
    "        # Escape the emoticon pattern to handle special characters\n",
    "        escaped_emoticon = re.escape(emoticon)\n",
    "        # Build the regex pattern to match emoticons not part of words\n",
    "        pattern = r'(?<!\\w)' + escaped_emoticon + r'(?!\\w)'\n",
    "        # Replace the emoticon with the description, ignoring case\n",
    "        clean_text = re.sub(pattern, f\"{description}\", clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "273f35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trade_mark(text):\n",
    "    \"\"\"Replace 'trade_mark' with a single quote (').\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace 'trade_mark' with \"'\"\n",
    "    clean_text = text.replace(\"trade_mark\", \"'\")\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea892d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = ' '.join(text.split())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9693e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_fns = [\n",
    "    replace_emojis,\n",
    "    normalize_text,\n",
    "    decode_html_entities,\n",
    "    remove_html_tags,\n",
    "    replace_urls,\n",
    "    replace_emails,\n",
    "    replace_censored_swear_words,\n",
    "    replace_hindi_swears,\n",
    "    replace_mentions,\n",
    "    replace_at_symbol,\n",
    "    remove_hashtags,\n",
    "    expand_contractions,\n",
    "    clean_unicode_punct,\n",
    "    transliterate_devanagari,\n",
    "    remove_emoji_colons,\n",
    "    replace_ascii_emoticons,\n",
    "    replace_trade_mark,\n",
    "    remove_extra_whitespace\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13ce3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt:str) -> str:\n",
    "    for fn in clean_fns: txt = fn(txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3789a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\"non_sarcastic\": 0, \"sarcastic\": 1}\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p,r,f,_ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "    return acc, p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8595f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_8660\\438787462.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(MODEL_DIR, \"bilstm_state.pt\"), map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + SVM\n",
    "tfidf, svm = joblib.load(os.path.join(MODEL_DIR, \"tfidf_svm.joblib\"))\n",
    "\n",
    "# BiLSTM\n",
    "vocab_rows = np.load(os.path.join(MODEL_DIR, \"vocab.npy\"), allow_pickle=True)\n",
    "token2idx  = {row[0]: int(row[1]) for row in vocab_rows}   # keys → ints\n",
    "\n",
    "EMB_DIM = 300\n",
    "MAXLEN  = 60\n",
    "\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=EMB_DIM, hidden=128):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm  = torch.nn.LSTM(emb_dim, hidden, bidirectional=True, batch_first=True)\n",
    "        self.fc    = torch.nn.Linear(hidden*2, 2)\n",
    "    def forward(self, x):\n",
    "        e,_ = self.lstm(self.embed(x))\n",
    "        h   = torch.cat([e[:,-1,:128], e[:,0,128:]], dim=-1)  # simple concat last+first dir\n",
    "        return self.fc(h)\n",
    "\n",
    "bilstm = BiLSTM(len(vocab)).eval()\n",
    "state = torch.load(os.path.join(MODEL_DIR, \"bilstm_state.pt\"), map_location=\"cpu\")\n",
    "bilstm.load_state_dict(state)\n",
    "\n",
    "# mBERT baseline\n",
    "mbert_tok = AutoTokenizer.from_pretrained(os.path.join(MODEL_DIR, \"bert_mbert\"))\n",
    "mbert     = AutoModelForSequenceClassification.from_pretrained(\n",
    "               os.path.join(MODEL_DIR, \"bert_mbert\")).eval()\n",
    "\n",
    "# Fine-tuned HingBERT\n",
    "hing_tok  = AutoTokenizer.from_pretrained(HINGBERT_DIR)\n",
    "hingbert  = AutoModelForSequenceClassification.from_pretrained(HINGBERT_DIR).eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bilstm, mbert, hingbert = bilstm.to(device), mbert.to(device), hingbert.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "573a9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_encode(texts):\n",
    "    ids = []\n",
    "    unk = vocab.get(\"<unk>\", 1)\n",
    "    for t in texts:\n",
    "        toks = t.split()[:MAXLEN]\n",
    "        vec  = [vocab.get(tok, unk) for tok in toks] + [0]*(MAXLEN-len(toks))\n",
    "        ids.append(vec)\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_all(texts):\n",
    "    # returns dict of numpy predictions\n",
    "    cleaned = [clean(t) for t in texts]\n",
    "\n",
    "    # SVM\n",
    "    svm_pred = svm.predict(tfidf.transform(cleaned))\n",
    "\n",
    "    # BiLSTM\n",
    "    logits   = bilstm(bilstm_encode(cleaned).to(device))\n",
    "    bil_pred = torch.argmax(logits, 1).cpu().numpy()\n",
    "\n",
    "    # mBERT\n",
    "    mb_inp   = mbert_tok(cleaned, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    mb_pred  = torch.argmax(mbert(**mb_inp).logits, 1).cpu().numpy()\n",
    "\n",
    "    # HingBERT\n",
    "    hg_inp   = hing_tok(cleaned, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    hg_pred  = torch.argmax(hingbert(**hg_inp).logits, 1).cpu().numpy()\n",
    "\n",
    "    return {\"SVM\":svm_pred, \"BiLSTM\":bil_pred, \"mBERT\":mb_pred, \"HingBERT\":hg_pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcdd95a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|██████████| 200/200 [00:18<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {m: [] for m in [\"SVM\", \"BiLSTM\", \"mBERT\", \"HingBERT\"]}\n",
    "\n",
    "csv_paths = sorted(glob.glob(os.path.join(BATCH_DIR, \"labeled_batch_*.csv\")))\n",
    "for path in tqdm(csv_paths, desc=\"batches\"):\n",
    "    df = pd.read_csv(path).dropna(subset=[\"sarcasm_label\",\n",
    "                                          \"Transliterated_Comment\"])\n",
    "\n",
    "    # 1️ normalise + map to 0/1\n",
    "    lbl_series = (\n",
    "        df[\"sarcasm_label\"]\n",
    "        .astype(str).str.strip().str.lower()\n",
    "        .map(LABEL_MAP)                      # non_sarcastic → 0, sarcastic → 1\n",
    "    )\n",
    "\n",
    "    # 2️ keep only rows that have a valid mapped label\n",
    "    valid_mask = lbl_series.notna()\n",
    "    if valid_mask.sum() == 0:\n",
    "        continue                             # skip empty batch\n",
    "\n",
    "    y = lbl_series[valid_mask].astype(int).values\n",
    "    texts = df.loc[valid_mask, \"Transliterated_Comment\"].astype(str)\n",
    "\n",
    "    preds = predict_all(texts)\n",
    "\n",
    "    for m, p in preds.items():\n",
    "        results[m].append(metrics(y, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc2c5e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.6356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6356</td>\n",
       "      <td>0.7751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BiLSTM</th>\n",
       "      <td>0.4793</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4793</td>\n",
       "      <td>0.6437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mBERT</th>\n",
       "      <td>0.7761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7761</td>\n",
       "      <td>0.8724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HingBERT</th>\n",
       "      <td>0.8735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8735</td>\n",
       "      <td>0.9318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy  Precision  Recall      F1\n",
       "SVM         0.6356        1.0  0.6356  0.7751\n",
       "BiLSTM      0.4793        1.0  0.4793  0.6437\n",
       "mBERT       0.7761        1.0  0.7761  0.8724\n",
       "HingBERT    0.8735        1.0  0.8735  0.9318"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {m: np.array(scores).mean(axis=0)\n",
    "           for m, scores in results.items()}\n",
    "summary_df = (pd.DataFrame(summary,\n",
    "                           index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "              .T.round(4))\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
