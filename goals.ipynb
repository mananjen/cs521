{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c4fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, joblib, torch, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import glob, pandas as pd, re, tqdm, collections, json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re, torch\n",
    "from wordfreq import zipf_frequency\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import re, regex\n",
    "import html, emoji, contractions\n",
    "from ftfy import fix_text\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caaa733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEX_PATH = Path(\"roman_hindi_lexicon.txt\")\n",
    "# if LEX_PATH.exists() and LEX_PATH.stat().st_size > 100_000:\n",
    "#     print(\"‚úÖ large lexicon already present:\", LEX_PATH.resolve())\n",
    "# else:\n",
    "#     print(\"üîß building offline roman-Hindi lexicon from your dataset ‚Ä¶\")\n",
    "#     # --- reload LID once ---\n",
    "#     lid_name = \"papluca/xlm-roberta-base-language-detection\"\n",
    "#     lid_tok  = AutoTokenizer.from_pretrained(lid_name)\n",
    "#     lid_net  = AutoModelForSequenceClassification.from_pretrained(lid_name)\n",
    "\n",
    "#     word_re  = re.compile(r\"\\b[a-z]{3,}\\b\", flags=re.I)\n",
    "#     vowel_pat= re.compile(r\"(aa|ee|ii|oo|uu|ai|au)\", re.I)\n",
    "\n",
    "#     def is_hinglish_candidate(tok):\n",
    "#         return vowel_pat.search(tok) is not None\n",
    "\n",
    "#     counter = collections.Counter()\n",
    "\n",
    "#     csv_paths = glob.glob(r\"labeled_batches\\labeled_batch_*.csv\")\n",
    "#     for path in tqdm.tqdm(csv_paths, desc=\"scanning\"):\n",
    "#         df = pd.read_csv(path, usecols=[\"Transliterated_Comment\"])\n",
    "#         for text in df[\"Transliterated_Comment\"].astype(str):\n",
    "#             for tok in word_re.findall(text.lower()):\n",
    "#                 if is_hinglish_candidate(tok):\n",
    "#                     counter[tok] += 1\n",
    "\n",
    "#     # keep top 15k candidates\n",
    "#     candidates = [w for w,_ in counter.most_common(15_000)]\n",
    "\n",
    "#     # batch-predict language\n",
    "#     roman_hi = set()\n",
    "#     B = 64\n",
    "#     for i in tqdm.tqdm(range(0, len(candidates), B), desc=\"LID\"):\n",
    "#         chunk = candidates[i:i+B]\n",
    "#         inputs= lid_tok(chunk, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         with torch.inference_mode():\n",
    "#             probs = F.softmax(lid_net(**inputs).logits, dim=-1)\n",
    "#         idxs  = probs.argmax(dim=-1)\n",
    "#         langs = [lid_net.config.id2label[j.item()] for j in idxs]\n",
    "#         confs = probs.max(dim=-1).values\n",
    "#         for w,l,c in zip(chunk, langs, confs):\n",
    "#             if l in (\"hi\",\"ur\") and c > 0.60:\n",
    "#                 roman_hi.add(w)\n",
    "\n",
    "#     # add your manual extra list\n",
    "#     roman_hi.update(\"\"\"\n",
    "#         kya haan nahi kaise kyu kyun bhai kitna kyon chal sahi banda khud\n",
    "#         sab theek acha accha pyaar dil zindagi\n",
    "#     \"\"\".split())\n",
    "\n",
    "#     # save\n",
    "#     LEX_PATH.write_text(\"\\n\".join(sorted(roman_hi)))\n",
    "#     print(f\"‚úÖ saved {len(roman_hi):,} roman-Hindi words ‚Üí {LEX_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca751e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lid_name = \"papluca/xlm-roberta-base-language-detection\"\n",
    "lid_tok  = AutoTokenizer.from_pretrained(lid_name)\n",
    "lid_net  = AutoModelForSequenceClassification.from_pretrained(lid_name)\n",
    "\n",
    "word_re = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
    "DEV_START, DEV_END = 0x0900, 0x097F\n",
    "\n",
    "roman_hi = set(open(\"roman_hindi_lexicon.txt\").read().split())\n",
    "\n",
    "def is_devanagari(tok: str) -> bool:\n",
    "    return any(DEV_START <= ord(c) <= DEV_END for c in tok)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def hf_lang(token):\n",
    "    inputs = lid_tok(token, return_tensors=\"pt\")\n",
    "    logits = lid_net(**inputs).logits\n",
    "    probs  = F.softmax(logits, dim=-1)\n",
    "    idx    = probs.argmax().item()\n",
    "    lang   = lid_net.config.id2label[idx]\n",
    "    return lang, probs[0, idx].item()       # ('hi', 0.78)\n",
    "\n",
    "def token_is_hindi(tok: str) -> bool:\n",
    "    tok_lc = tok.lower()\n",
    "    if is_devanagari(tok):                     # Stage 1\n",
    "        return True\n",
    "    if tok_lc in roman_hi:                     # Stage 2\n",
    "        return True\n",
    "    lang, p = hf_lang(tok_lc)            # Stage 3\n",
    "    if lang in {\"hi\", \"ur\"} and p > 0.5:\n",
    "        return True\n",
    "    return zipf_frequency(tok_lc, \"en\") < 2.5 # Stage 4 (rarity)\n",
    "\n",
    "def hindi_ratio(text: str) -> float:\n",
    "    toks = word_re.findall(text)\n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    hi = sum(token_is_hindi(t) for t in toks)\n",
    "    return hi / len(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6d5957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "print(hindi_ratio(\"kya haal hai?\"))                  # ‚Üí ~1.0\n",
    "print(hindi_ratio(\"good morning everyone\"))          # ‚Üí ~0.0\n",
    "print(hindi_ratio(\"kya bol rahe ho boss good job\"))   # ‚Üí ~0.4-0.6 (mixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355df916",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = r\"D:/University of Illinois Chicago/Classes/CS521/cs521\"\n",
    "BATCH_DIR = os.path.join(ROOT, \"labeled_batches\")\n",
    "BASE_DIR  = os.path.join(ROOT, \"baseline_models\")\n",
    "HING_DIR  = os.path.join(ROOT, \"sarcasm_hingbert_model\")\n",
    "DEVANAGARI_TO_ROMAN = True\n",
    "LATIN_BLOCK= r'\\p{Latin}'\n",
    "DEVANAGARI_BLOCK = r'\\p{Devanagari}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c1446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_swear_words = {\n",
    "    # Handles s!@#, s#!t, s**t, etc., variations of \"shit\"\n",
    "    r'\\b[Ss][^A-Za-z\\s]{3,4}\\b': 'shit',\n",
    "    r'[Ss][\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t]+': 'shit',\n",
    "    r'[Ss][#!@$%^&*+\\-=\\[\\]{};:\\'\",.<>?/\\\\|_~`]{2,}[Tt]\\b': 'shit',\n",
    "\n",
    "\n",
    "    # Handles f*ck, f@ck, f**k, f!ck, F*#k, F***, etc., variations of \"fuck\"\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+': 'fuck',\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[c][k]': 'fuck',                # Handles f*ck\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[k]': 'fuck',                   # Handles f***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*#\\!]{1,}[c]?[kz]?': 'fuck',       # Handles F*#k, F***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\w@#$%^&*]+ing\\b': 'fucking',        # Hnadles f#%^ing\n",
    "\n",
    "    # Handles b*tch, b!tch, b1tch, etc., variations of \"bitch\"\n",
    "    r'b[\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[h]+': 'bitch',\n",
    "\n",
    "    # Handles \"fuck\" with various suffixes (e.g., \"f***'z\" -> \"fucks\")\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+[\\'‚Äô]?[zs]?': 'fucks',\n",
    "\n",
    "    # Handles muthaf*ckin, motherf***ing, etc., variations of \"motherfucking\"\n",
    "    r'm[ou]*th[\\!\\@\\#\\$\\%\\^\\&\\*]*[a][\\!\\@\\#\\$\\%\\^\\&\\*]*f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k][\\!\\@\\#\\$\\%\\^\\&\\*]*[i][\\!\\@\\#\\$\\%\\^\\&\\*]*[n][\\!\\@\\#\\$\\%\\^\\&\\*]*': 'motherfucking',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d60c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_swear_map = {\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúmadarchod‚Äù ‚Äî‚Äî\n",
    "    # Roman + common leet/censor + Devanagari\n",
    "    r'\\b(m[a@]d[ae]?[r]?[\\W_]*ch[o0]d[a@]?|‡§Æ[‡§æ‡§Ö]*‡§¶‡§∞[‡§ö‡§ö]‡•ã‡§¶|‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶)\\b': 'madarchod',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúbhenchod / behenchod‚Äù ‚Äî‚Äî\n",
    "    r'\\b(b[h]*e+h[e]*n[\\W_]*ch[o0]d[a@]?|‡§≠‡•á‡§Ç‡§ö‡•ã‡§¶|‡§¨‡§π[‡•á‡•á]‡§®[‡§ö‡§ö]‡•ã‡§¶|‡§¨‡§π‡§®‡§ö‡•ã‡§¶)\\b': 'bhenchod',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúchutiya / ch**iya‚Äù ‚Äî‚Äî\n",
    "    r'\\b(c[h]*u+t+i+y*a+|ch[u*]+t+iy?[ae]+|‡§ö‡•Ç‡§§‡§ø‡§Ø‡§æ|‡§ö‡•Å‡§§‡§ø‡§Ø‡§æ|‡§õ‡•Ç‡§§‡§ø‡§Ø‡§æ)\\b': 'chutiya',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúchu***‚Äù shortened form ‚Äî‚Äî\n",
    "    r'\\b(ch[\\W_]*u[\\W_]*t[\\W_]*\\*{2,})\\b': 'chut***',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúgandu / gaandu / gaand‚Äù ‚Äî‚Äî\n",
    "    r'\\b(g[a@]a*n+d[u]?[aei]?|‡§ó‡§æ‡§Ç*‡§°‡•Ç?|‡§ó‡§æ‡§£‡•ç‡§°‡•Ç?)\\b': 'gandu',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúgaand‚Äù stand‚Äëalone  ‚Äî‚Äî\n",
    "    r'\\b(ga+a+nd+|‡§ó‡§æ‡§Ç‡§°|‡§ó‡§æ‡§Ç‡§°|‡§ó‡§æ‡§£‡•ç‡§°)\\b': 'gaand',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúlauda / laudae / laude‚Äù ‚Äî‚Äî \n",
    "    r'\\b(l[a@]u+[d]+[aei]*|‡§≤‡•å‡§°‡§º‡§æ|‡§≤‡§Ç‡§°|‡§≤‡•Å‡§Ç‡§°)\\b': 'lauda',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúlund‚Äù ‚Äî‚Äî\n",
    "    r'\\b(l[u]n+d|‡§≤‡•Å‡§Ç‡§°|‡§≤‡§Ç‡§°)\\b': 'lund',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúrandi / r@ndi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(r[a@]n+d[iy]+|‡§∞‡§Ç‡§°‡•Ä|‡§∞‡§Ç‡§°‡§º‡•Ä)\\b': 'randi',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúharami / haramzada / haramzadi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(h[a@]r+a+m[iy]*|‡§π‡§∞‡§æ‡§Æ‡•Ä)\\b': 'harami',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[a@]|‡§π‡§∞‡§æ‡§Æ‡§ú‡§º‡§æ‡§¶‡§æ|‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡§æ)\\b': 'haramzada',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[iy]|‡§π‡§∞‡§æ‡§Æ‡§ú‡§º‡§æ‡§¶‡•Ä|‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡•Ä)\\b': 'haramzadi',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúkamine / kamina / kaminey‚Äù ‚Äî‚Äî\n",
    "    r'\\b(k[a@]m+i+n[e]?[yie]*|‡§ï‡§Æ‡•Ä‡§®‡§æ|‡§ï‡§Æ‡•Ä‡§®‡•á|‡§ï‡§Æ‡•Ä‡§®‡•Ä)\\b': 'kamine',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúkutta / kutte / kuttiya‚Äù ‚Äî‚Äî\n",
    "    r'\\b(k[u]t+t[a@e]*|‡§ï‡•Å‡§§‡•ç‡§§‡§æ|‡§ï‡•Å‡§§‡•ç‡§§‡•á|‡§ï‡•Å‡§§‡•á)\\b': 'kutta',\n",
    "    r'\\b(k[u]t+t[i]y?a+|‡§ï‡•Å‡§§‡§ø‡§Ø‡§æ|‡§ï‡•Å‡§§‡•ç‡§§‡•Ä)\\b': 'kutiya',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúsaala / saale / sali‚Äù ‚Äî‚Äî\n",
    "    r'\\b(s+a+a*l+a+|‡§∏‡§æ‡§≤‡§æ|‡§∏‡§æ‡§≤‡•á)\\b': 'saala',\n",
    "    r'\\b(s+a+l+i+|‡§∏‡§æ‡§≤‡•Ä)\\b': 'sali',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúbakchod / bakchodi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(b[a@]k+ch[o0]d[iy]*|‡§¨‡§ï‡§ö‡•ã‡§¶|‡§¨‡§ï‡§ö‡•ã‡§¶‡•Ä)\\b' : 'bakchod',\n",
    "\n",
    "    # ‚Äî‚Äî milder insults ‚Äî‚Äî\n",
    "    r'\\b(g[a@]dh[a@]a+|‡§ó‡§ß‡§æ|‡§ó‡§ß‡•á)\\b': 'gadha',\n",
    "\n",
    "    # ‚Äî‚Äî catch‚Äëall ‚Äúf**k‚Äù in Roman Hindi sentences ‚Äî‚Äî\n",
    "    r'\\b(f[\\W_]*u[\\W_]*c[\\W_]*k+)\\b': 'fuck',\n",
    "\n",
    "    # ‚Äî‚Äî emojis or censor blocks containing DevanƒÅgarƒ´ letters and stars\n",
    "    r'[\\u0900-\\u097F]\\*{2,}[\\u0900-\\u097F]*': 'censored_hindi_word',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cd8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_emoticons = {\n",
    "    ':-)': 'smiley_face',\n",
    "    ':)': 'smiley_face',\n",
    "    ':-D': 'grinning_face',\n",
    "    ':D': 'grinning_face',\n",
    "    ':-(': 'sad_face',\n",
    "    ':(': 'sad_face',\n",
    "    ':-P': 'playful_face',\n",
    "    ':P': 'playful_face',\n",
    "    ':-p': 'playful_face',\n",
    "    ':p': 'playful_face',\n",
    "    ';-)': 'winking_face',\n",
    "    ';)': 'winking_face',\n",
    "    ':-O': 'surprised_face',\n",
    "    ':O': 'surprised_face',\n",
    "    ':-o': 'surprised_face',\n",
    "    ':o': 'surprised_face',\n",
    "    r':-/': 'skeptical_face',\n",
    "    r':/': 'skeptical_face',\n",
    "    r':-\\|': 'neutral_face',\n",
    "    r':\\|': 'neutral_face',\n",
    "    '<3': 'heart',\n",
    "    '^_^': 'happy_face',\n",
    "    '-_-': 'expressionless_face',\n",
    "    'o_O': 'confused_face',\n",
    "    'O_o': 'confused_face',\n",
    "    'o_o': 'confused_face',\n",
    "    '>_<': 'frustrated_face',\n",
    "    'O.o': 'confused_face',\n",
    "    'o.O': 'confused_face',\n",
    "    '0_o': 'confused_face',\n",
    "    'o_0': 'confused_face',\n",
    "    'xD': 'laughing_face',\n",
    "    'XD': 'laughing_face',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1031ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text) if isinstance(text, str) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b01aea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        # Attempt to encode to 'latin1' and decode to 'utf-8' to fix encoding issues\n",
    "        clean_text = text.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        # Fallback: ignore encoding errors if they occur\n",
    "        clean_text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1087be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_html_entities(text):\n",
    "    \"\"\"Decode HTML entities in text, such as '&amp;' to '&'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Use html.unescape to decode all HTML entities\n",
    "    clean_text = html.unescape(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5067ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = re.sub(r'</?[^>]+>', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae711707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_urls(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match URLs\n",
    "    url_pattern = r'(http[s]?://\\S+|www\\.\\S+)'\n",
    "    clean_text = re.sub(url_pattern, ' SomeWebLink ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "196d0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emails(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match email addresses\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "    clean_text = re.sub(email_pattern, ' EMAILADDRESS ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65dfe5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_censored_swear_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = text\n",
    "    for pattern, replacement in censored_swear_words.items():\n",
    "        # Do not add word boundaries if pattern already contains them\n",
    "        if pattern.startswith(r'\\b') and pattern.endswith(r'\\b'):\n",
    "            clean_pattern = pattern\n",
    "        else:\n",
    "            clean_pattern = r'\\b' + pattern + r'\\b'\n",
    "        clean_text = re.sub(clean_pattern, replacement, clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d306c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hindi_swears(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    out = text\n",
    "    for pat, repl in hindi_swear_map.items():\n",
    "        out = regex.sub(pat, repl, out, flags=regex.IGNORECASE)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d47c6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mentions(text):\n",
    "    \"\"\"Replace mentions with 'SomeTaggedAccount', including when attached to other words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match @ followed by word characters\n",
    "    # Replace mentions with spaces around to prevent word merging\n",
    "    mention_pattern = r'@([A-Za-z0-9_]+)'\n",
    "    clean_text = re.sub(mention_pattern, ' SomeTaggedAccount ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f871007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_at_symbol(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace '@' when used as 'at'\n",
    "    # Conditions:\n",
    "    # - '@' followed by a space\n",
    "    # - '@' at the end of a string\n",
    "    # - '@' followed by a number or time format\n",
    "    # - '@' followed by a capitalized word (assuming location or time)\n",
    "    clean_text = re.sub(r'@\\s', 'at ', text)\n",
    "    clean_text = re.sub(r'@$', 'at', clean_text)\n",
    "    clean_text = re.sub(r'@(?=\\d)', 'at ', clean_text)\n",
    "    clean_text = re.sub(r'@ (?=[A-Z])', 'at ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d25f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove '#' symbol in various contexts\n",
    "    clean_text = re.sub(r'(^|\\s)#(\\w*[^\\s\\w]?)', r'\\1\\2', text)\n",
    "    clean_text = re.sub(r'(\\w+)#([^\\s\\w]?)', r'\\1\\2', clean_text)\n",
    "    clean_text = re.sub(r'([^\\s\\w])#', r'\\1', clean_text)\n",
    "\n",
    "    # Remove any remaining standalone '#' symbols\n",
    "    clean_text = re.sub(r'(?<=\\s)#(?=\\s|$)|(?<=^)#(?=\\s|$)', '', clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b1d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    clean_text = ' '.join(expanded_words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84faf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicode_punct(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation *except* ? ! . , and keep both Latin & Devanagari letters.\n",
    "    Any other symbol is dropped.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # keep letters, numbers, spaces and selected punctuation\n",
    "    pattern = rf'[^\\s{LATIN_BLOCK}{DEVANAGARI_BLOCK}0-9\\?\\!\\.,]'\n",
    "    return regex.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b896b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_devanagari(text: str) -> str:\n",
    "    if not DEVANAGARI_TO_ROMAN or not isinstance(text, str):\n",
    "        return text\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f834154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_colons(text):\n",
    "    \"\"\"Remove colons around emoji descriptions in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match emoji descriptions with colons\n",
    "    pattern = r':([a-zA-Z0-9_+-]+):'\n",
    "    # Replace matches with the emoji description without colons\n",
    "    clean_text = re.sub(pattern, r'\\1', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aba36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ascii_emoticons(text):\n",
    "    \"\"\"Replace ASCII emoticons in text with their descriptions.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    clean_text = text\n",
    "    for emoticon, description in ascii_emoticons.items():\n",
    "        # Escape the emoticon pattern to handle special characters\n",
    "        escaped_emoticon = re.escape(emoticon)\n",
    "        # Build the regex pattern to match emoticons not part of words\n",
    "        pattern = r'(?<!\\w)' + escaped_emoticon + r'(?!\\w)'\n",
    "        # Replace the emoticon with the description, ignoring case\n",
    "        clean_text = re.sub(pattern, f\"{description}\", clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37ed7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trade_mark(text):\n",
    "    \"\"\"Replace 'trade_mark' with a single quote (').\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace 'trade_mark' with \"'\"\n",
    "    clean_text = text.replace(\"trade_mark\", \"'\")\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "659de0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = ' '.join(text.split())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2766377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_fns = [replace_emojis, normalize_text, decode_html_entities, remove_html_tags,\n",
    "             replace_urls, replace_emails, replace_censored_swear_words,\n",
    "             replace_hindi_swears, replace_mentions, replace_at_symbol,\n",
    "             remove_hashtags, expand_contractions, clean_unicode_punct,\n",
    "             transliterate_devanagari, remove_emoji_colons,\n",
    "             replace_ascii_emoticons, replace_trade_mark, remove_extra_whitespace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5553b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt:str):\n",
    "    for fn in clean_fns: txt = fn(txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d9646f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, svm = joblib.load(os.path.join(BASE_DIR, \"tfidf_svm.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdf76448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_7648\\3531386633.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bilstm.load_state_dict(torch.load(os.path.join(BASE_DIR, \"bilstm_state.pt\"), map_location=\"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embed): Embedding(5986, 300, padding_idx=0)\n",
       "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, re\n",
    "rows = np.load(os.path.join(BASE_DIR, \"vocab.npy\"), allow_pickle=True)\n",
    "token2idx  = {k:int(v) for k,v in rows.tolist()}\n",
    "EMB_DIM, MAXLEN = 300, 60\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden=128):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden                       # ‚Üê store\n",
    "        self.embed  = torch.nn.Embedding(vocab_size, EMB_DIM, padding_idx=0)\n",
    "        self.lstm   = torch.nn.LSTM(EMB_DIM, hidden, bidirectional=True, batch_first=True)\n",
    "        self.fc     = torch.nn.Linear(hidden * 2, 2)\n",
    "    def forward(self, x):\n",
    "        e, _ = self.lstm(self.embed(x))\n",
    "        h = torch.cat([e[:, -1, :self.hidden],      # ‚Üê use self.hidden\n",
    "                       e[:,  0,  self.hidden:]], 1)\n",
    "        return self.fc(h)\n",
    "bilstm = BiLSTM(len(token2idx))\n",
    "bilstm.load_state_dict(torch.load(os.path.join(BASE_DIR, \"bilstm_state.pt\"), map_location=\"cpu\"))\n",
    "bilstm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66d83232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_encode(texts):\n",
    "    ids, unk = [], token2idx.get(\"<unk>\",1)\n",
    "    for t in texts:\n",
    "        toks = t.split()[:MAXLEN]\n",
    "        vec  = [token2idx.get(tok, unk) for tok in toks] + [0]*(MAXLEN-len(toks))\n",
    "        ids.append(vec)\n",
    "    return torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cdb3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_path = os.path.join(BASE_DIR, \"bert_mbert\")\n",
    "mbert_tok = AutoTokenizer.from_pretrained(mb_path)\n",
    "mbert_net = AutoModelForSequenceClassification.from_pretrained(mb_path).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd593870",
   "metadata": {},
   "outputs": [],
   "source": [
    "hing_tok = AutoTokenizer.from_pretrained(HING_DIR)\n",
    "hing_net = AutoModelForSequenceClassification.from_pretrained(HING_DIR).eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bilstm, mbert_net, hing_net = bilstm.to(device), mbert_net.to(device), hing_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f860ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72d0bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(texts):\n",
    "    cleaned = [clean(t) for t in texts]\n",
    "\n",
    "    # SVM\n",
    "    svm_pred = svm.predict(tfidf.transform(cleaned))\n",
    "\n",
    "    # BiLSTM\n",
    "    with torch.no_grad():\n",
    "        logits = bilstm(bilstm_encode(cleaned).to(device))\n",
    "        bil_pred = logits.argmax(1).cpu().numpy()\n",
    "\n",
    "    # mBERT\n",
    "    with torch.no_grad():\n",
    "        mb = mbert_tok(cleaned, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "        mb_pred = mbert_net(**mb).logits.argmax(1).cpu().numpy()\n",
    "\n",
    "    # HingBERT\n",
    "    with torch.no_grad():\n",
    "        hg = hing_tok(cleaned, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "        hg_pred = hing_net(**hg).logits.argmax(1).cpu().numpy()\n",
    "\n",
    "    return {\"SVM\":svm_pred, \"BiLSTM\":bil_pred, \"mBERT\":mb_pred, \"HingBERT\":hg_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "899faf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges  = [0, .2, .4, .6, .8, 1.01]\n",
    "labels = [\"0-20%\", \"20-40%\", \"40-60%\", \"60-80%\", \"80-100%\"]\n",
    "def get_bucket(r):           # r ‚àà [0,1]\n",
    "    for i in range(len(edges)-1):\n",
    "        if edges[i] <= r < edges[i+1]:\n",
    "            return labels[i]\n",
    "\n",
    "bucket_pairs = {m:{b:[] for b in labels} for m in [\"SVM\",\"BiLSTM\",\"mBERT\",\"HingBERT\"]}\n",
    "\n",
    "LABEL_MAP = {\"non_sarcastic\":0, \"sarcastic\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c536c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [27:22<00:00,  8.21s/it]\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(glob.glob(os.path.join(BATCH_DIR, \"labeled_batch_*.csv\")))\n",
    "for p in tqdm(paths, desc=\"batches\"):\n",
    "    df = pd.read_csv(p).dropna(subset=[\"sarcasm_label\",\"Transliterated_Comment\"])\n",
    "    lbls = df[\"sarcasm_label\"].str.strip().str.lower().map(LABEL_MAP)\n",
    "    mask = lbls.notna()\n",
    "    texts = df.loc[mask,\"Transliterated_Comment\"].astype(str)\n",
    "    y_true= lbls[mask].astype(int).values\n",
    "\n",
    "    ratios = texts.map(hindi_ratio).tolist()\n",
    "    buckets= [get_bucket(r) for r in ratios]\n",
    "\n",
    "    preds = predict_all(texts.tolist())\n",
    "    for m, y_pred in preds.items():\n",
    "        for b, yt, yp in zip(buckets, y_true, y_pred):\n",
    "            bucket_pairs[m][b].append((yt, yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f20604a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM</th>\n",
       "      <th>BiLSTM</th>\n",
       "      <th>mBERT</th>\n",
       "      <th>HingBERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-20%</th>\n",
       "      <td>0.635</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-40%</th>\n",
       "      <td>0.664</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-60%</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60-80%</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80-100%</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SVM  BiLSTM  mBERT  HingBERT\n",
       "0-20%    0.635   0.317  0.534     0.878\n",
       "20-40%   0.664   0.565  0.771     0.904\n",
       "40-60%   0.710   0.597  0.858     0.905\n",
       "60-80%   0.796   0.671  0.891     0.944\n",
       "80-100%  0.795   0.662  0.885     0.934"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f1(pairs):\n",
    "    if not pairs: return np.nan\n",
    "    yt, yp = zip(*pairs)\n",
    "    _,_,f,_ = precision_recall_fscore_support(yt, yp, average=\"binary\")\n",
    "    return f\n",
    "\n",
    "data = {m:[f1(bucket_pairs[m][b]) for b in labels] for m in bucket_pairs}\n",
    "bucket_df = pd.DataFrame(data, index=labels)\n",
    "display(bucket_df.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
