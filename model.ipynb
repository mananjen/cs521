{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "567f2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import re, regex\n",
    "import html, emoji, contractions\n",
    "from ftfy import fix_text\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9b8f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af457411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1 CUDA: 12.1\n",
      "GPU detected: NVIDIA GeForce RTX 4090\n",
      "CUDA OK? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
    "print(\"GPU detected:\", torch.cuda.get_device_name(0))\n",
    "print(\"CUDA OK?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e19c3d",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cebc7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVANAGARI_TO_ROMAN = True\n",
    "LATIN_BLOCK= r'\\p{Latin}'\n",
    "DEVANAGARI_BLOCK = r'\\p{Devanagari}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d9f37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "censored_swear_words = {\n",
    "    # Handles s!@#, s#!t, s**t, etc., variations of \"shit\"\n",
    "    r'\\b[Ss][^A-Za-z\\s]{3,4}\\b': 'shit',\n",
    "    r'[Ss][\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t]+': 'shit',\n",
    "    r'[Ss][#!@$%^&*+\\-=\\[\\]{};:\\'\",.<>?/\\\\|_~`]{2,}[Tt]\\b': 'shit',\n",
    "\n",
    "\n",
    "    # Handles f*ck, f@ck, f**k, f!ck, F*#k, F***, etc., variations of \"fuck\"\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+': 'fuck',\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[c][k]': 'fuck',                # Handles f*ck\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*]+[k]': 'fuck',                   # Handles f***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\*#\\!]{1,}[c]?[kz]?': 'fuck',       # Handles F*#k, F***\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[\\w@#$%^&*]+ing\\b': 'fucking',        # Hnadles f#%^ing\n",
    "\n",
    "    # Handles b*tch, b!tch, b1tch, etc., variations of \"bitch\"\n",
    "    r'b[\\!\\@\\#\\$\\%\\^\\&\\*]*[i1\\|][\\!\\@\\#\\$\\%\\^\\&\\*]*[t][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[h]+': 'bitch',\n",
    "\n",
    "    # Handles \"fuck\" with various suffixes (e.g., \"f***'z\" -> \"fucks\")\n",
    "    r'f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k]+[\\'‚Äô]?[zs]?': 'fucks',\n",
    "\n",
    "    # Handles muthaf*ckin, motherf***ing, etc., variations of \"motherfucking\"\n",
    "    r'm[ou]*th[\\!\\@\\#\\$\\%\\^\\&\\*]*[a][\\!\\@\\#\\$\\%\\^\\&\\*]*f[\\!\\@\\#\\$\\%\\^\\&\\*]*[u][\\!\\@\\#\\$\\%\\^\\&\\*]*[c][\\!\\@\\#\\$\\%\\^\\&\\*]*[k][\\!\\@\\#\\$\\%\\^\\&\\*]*[i][\\!\\@\\#\\$\\%\\^\\&\\*]*[n][\\!\\@\\#\\$\\%\\^\\&\\*]*': 'motherfucking',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5a148fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_swear_map = {\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúmadarchod‚Äù ‚Äî‚Äî (mother‚Äë****er)\n",
    "    # Roman + common leet/censor + Devanagari\n",
    "    r'\\b(m[a@]d[ae]?[r]?[\\W_]*ch[o0]d[a@]?|‡§Æ[‡§æ‡§Ö]*‡§¶‡§∞[‡§ö‡§ö]‡•ã‡§¶|‡§Æ‡§æ‡§¶‡§∞‡§ö‡•ã‡§¶)\\b': 'madarchod',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúbhenchod / behenchod‚Äù ‚Äî‚Äî\n",
    "    r'\\b(b[h]*e+h[e]*n[\\W_]*ch[o0]d[a@]?|‡§≠‡•á‡§Ç‡§ö‡•ã‡§¶|‡§¨‡§π[‡•á‡•á]‡§®[‡§ö‡§ö]‡•ã‡§¶|‡§¨‡§π‡§®‡§ö‡•ã‡§¶)\\b': 'bhenchod',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúchutiya / ch**iya‚Äù ‚Äî‚Äî\n",
    "    r'\\b(c[h]*u+t+i+y*a+|ch[u*]+t+iy?[ae]+|‡§ö‡•Ç‡§§‡§ø‡§Ø‡§æ|‡§ö‡•Å‡§§‡§ø‡§Ø‡§æ|‡§õ‡•Ç‡§§‡§ø‡§Ø‡§æ)\\b': 'chutiya',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúchu***‚Äù shortened form (e.g. chut** / ch****) ‚Äî‚Äî\n",
    "    r'\\b(ch[\\W_]*u[\\W_]*t[\\W_]*\\*{2,})\\b': 'chut***',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúgandu / gaandu / gaand‚Äù ‚Äî‚Äî\n",
    "    r'\\b(g[a@]a*n+d[u]?[aei]?|‡§ó‡§æ‡§Ç*‡§°‡•Ç?|‡§ó‡§æ‡§£‡•ç‡§°‡•Ç?)\\b': 'gandu',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúgaand‚Äù stand‚Äëalone  ‚Äî‚Äî\n",
    "    r'\\b(ga+a+nd+|‡§ó‡§æ‡§Ç‡§°|‡§ó‡§æ‡§Ç‡§°|‡§ó‡§æ‡§£‡•ç‡§°)\\b': 'gaand',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúlauda / laudae / laude‚Äù ‚Äî‚Äî \n",
    "    r'\\b(l[a@]u+[d]+[aei]*|‡§≤‡•å‡§°‡§º‡§æ|‡§≤‡§Ç‡§°|‡§≤‡•Å‡§Ç‡§°)\\b': 'lauda',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúlund‚Äù ‚Äî‚Äî\n",
    "    r'\\b(l[u]n+d|‡§≤‡•Å‡§Ç‡§°|‡§≤‡§Ç‡§°)\\b': 'lund',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúrandi / r@ndi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(r[a@]n+d[iy]+|‡§∞‡§Ç‡§°‡•Ä|‡§∞‡§Ç‡§°‡§º‡•Ä)\\b': 'randi',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúharami / haramzada / haramzadi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(h[a@]r+a+m[iy]*|‡§π‡§∞‡§æ‡§Æ‡•Ä)\\b': 'harami',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[a@]|‡§π‡§∞‡§æ‡§Æ‡§ú‡§º‡§æ‡§¶‡§æ|‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡§æ)\\b': 'haramzada',\n",
    "    r'\\b(h[a@]r+a+m[z$]?a+d[iy]|‡§π‡§∞‡§æ‡§Æ‡§ú‡§º‡§æ‡§¶‡•Ä|‡§π‡§∞‡§æ‡§Æ‡§ú‡§æ‡§¶‡•Ä)\\b': 'haramzadi',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúkamine / kamina / kaminey‚Äù ‚Äî‚Äî\n",
    "    r'\\b(k[a@]m+i+n[e]?[yie]*|‡§ï‡§Æ‡•Ä‡§®‡§æ|‡§ï‡§Æ‡•Ä‡§®‡•á|‡§ï‡§Æ‡•Ä‡§®‡•Ä)\\b': 'kamine',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúkutta / kutte / kuttiya‚Äù ‚Äî‚Äî\n",
    "    r'\\b(k[u]t+t[a@e]*|‡§ï‡•Å‡§§‡•ç‡§§‡§æ|‡§ï‡•Å‡§§‡•ç‡§§‡•á|‡§ï‡•Å‡§§‡•á)\\b': 'kutta',\n",
    "    r'\\b(k[u]t+t[i]y?a+|‡§ï‡•Å‡§§‡§ø‡§Ø‡§æ|‡§ï‡•Å‡§§‡•ç‡§§‡•Ä)\\b': 'kutiya',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúsaala / saale / sali‚Äù ‚Äî‚Äî (brother‚Äëin‚Äëlaw, used as slur)\n",
    "    r'\\b(s+a+a*l+a+|‡§∏‡§æ‡§≤‡§æ|‡§∏‡§æ‡§≤‡•á)\\b': 'saala',\n",
    "    r'\\b(s+a+l+i+|‡§∏‡§æ‡§≤‡•Ä)\\b': 'sali',\n",
    "\n",
    "    # ‚Äî‚Äî ‚Äúbakchod / bakchodi‚Äù ‚Äî‚Äî\n",
    "    r'\\b(b[a@]k+ch[o0]d[iy]*|‡§¨‡§ï‡§ö‡•ã‡§¶|‡§¨‡§ï‡§ö‡•ã‡§¶‡•Ä)\\b' : 'bakchod',\n",
    "\n",
    "    # ‚Äî‚Äî milder insults ‚Äî‚Äî\n",
    "    r'\\b(g[a@]dh[a@]a+|‡§ó‡§ß‡§æ|‡§ó‡§ß‡•á)\\b': 'gadha',\n",
    "\n",
    "    # ‚Äî‚Äî catch‚Äëall ‚Äúf**k‚Äù in Roman Hindi sentences ‚Äî‚Äî\n",
    "    r'\\b(f[\\W_]*u[\\W_]*c[\\W_]*k+)\\b': 'fuck',\n",
    "\n",
    "    # ‚Äî‚Äî emojis or censor blocks containing DevanƒÅgarƒ´ letters and stars\n",
    "    r'[\\u0900-\\u097F]\\*{2,}[\\u0900-\\u097F]*': 'censored_hindi_word',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f74a91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_emoticons = {\n",
    "    ':-)': 'smiley_face',\n",
    "    ':)': 'smiley_face',\n",
    "    ':-D': 'grinning_face',\n",
    "    ':D': 'grinning_face',\n",
    "    ':-(': 'sad_face',\n",
    "    ':(': 'sad_face',\n",
    "    ':-P': 'playful_face',\n",
    "    ':P': 'playful_face',\n",
    "    ':-p': 'playful_face',\n",
    "    ':p': 'playful_face',\n",
    "    ';-)': 'winking_face',\n",
    "    ';)': 'winking_face',\n",
    "    ':-O': 'surprised_face',\n",
    "    ':O': 'surprised_face',\n",
    "    ':-o': 'surprised_face',\n",
    "    ':o': 'surprised_face',\n",
    "    r':-/': 'skeptical_face',\n",
    "    r':/': 'skeptical_face',\n",
    "    r':-\\|': 'neutral_face',\n",
    "    r':\\|': 'neutral_face',\n",
    "    '<3': 'heart',\n",
    "    '^_^': 'happy_face',\n",
    "    '-_-': 'expressionless_face',\n",
    "    'o_O': 'confused_face',\n",
    "    'O_o': 'confused_face',\n",
    "    'o_o': 'confused_face',\n",
    "    '>_<': 'frustrated_face',\n",
    "    'O.o': 'confused_face',\n",
    "    'o.O': 'confused_face',\n",
    "    '0_o': 'confused_face',\n",
    "    'o_0': 'confused_face',\n",
    "    'xD': 'laughing_face',\n",
    "    'XD': 'laughing_face',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91dc54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text):\n",
    "    return emoji.demojize(text) if isinstance(text, str) else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b294acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    try:\n",
    "        # Attempt to encode to 'latin1' and decode to 'utf-8' to fix encoding issues\n",
    "        clean_text = text.encode('latin1').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        # Fallback: ignore encoding errors if they occur\n",
    "        clean_text = text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4407d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_html_entities(text):\n",
    "    \"\"\"Decode HTML entities in text, such as '&amp;' to '&'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Use html.unescape to decode all HTML entities\n",
    "    clean_text = html.unescape(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fc69a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = re.sub(r'</?[^>]+>', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a514ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_urls(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match URLs\n",
    "    url_pattern = r'(http[s]?://\\S+|www\\.\\S+)'\n",
    "    clean_text = re.sub(url_pattern, ' SomeWebLink ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a43283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emails(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match email addresses\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "    clean_text = re.sub(email_pattern, ' EMAILADDRESS ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2379b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_censored_swear_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = text\n",
    "    for pattern, replacement in censored_swear_words.items():\n",
    "        # Do not add word boundaries if pattern already contains them\n",
    "        if pattern.startswith(r'\\b') and pattern.endswith(r'\\b'):\n",
    "            clean_pattern = pattern\n",
    "        else:\n",
    "            clean_pattern = r'\\b' + pattern + r'\\b'\n",
    "        clean_text = re.sub(clean_pattern, replacement, clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6008eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hindi_swears(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    out = text\n",
    "    for pat, repl in hindi_swear_map.items():\n",
    "        out = regex.sub(pat, repl, out, flags=regex.IGNORECASE)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0e3d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mentions(text):\n",
    "    \"\"\"Replace mentions with 'SomeTaggedAccount', including when attached to other words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern to match @ followed by word characters\n",
    "    # Replace mentions with spaces around to prevent word merging\n",
    "    mention_pattern = r'@([A-Za-z0-9_]+)'\n",
    "    clean_text = re.sub(mention_pattern, ' SomeTaggedAccount ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bd32e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_at_symbol(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace '@' when used as 'at'\n",
    "    # Conditions:\n",
    "    # - '@' followed by a space\n",
    "    # - '@' at the end of a string\n",
    "    # - '@' followed by a number or time format\n",
    "    # - '@' followed by a capitalized word (assuming location or time)\n",
    "    clean_text = re.sub(r'@\\s', 'at ', text)\n",
    "    clean_text = re.sub(r'@$', 'at', clean_text)\n",
    "    clean_text = re.sub(r'@(?=\\d)', 'at ', clean_text)\n",
    "    clean_text = re.sub(r'@ (?=[A-Z])', 'at ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42b172ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove '#' symbol in various contexts\n",
    "    clean_text = re.sub(r'(^|\\s)#(\\w*[^\\s\\w]?)', r'\\1\\2', text)\n",
    "    clean_text = re.sub(r'(\\w+)#([^\\s\\w]?)', r'\\1\\2', clean_text)\n",
    "    clean_text = re.sub(r'([^\\s\\w])#', r'\\1', clean_text)\n",
    "\n",
    "    # Remove any remaining standalone '#' symbols\n",
    "    clean_text = re.sub(r'(?<=\\s)#(?=\\s|$)|(?<=^)#(?=\\s|$)', '', clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db856743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    clean_text = ' '.join(expanded_words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28536932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicode_punct(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation *except* ? ! . , and keep both Latin & Devanagari letters.\n",
    "    Any other symbol is dropped.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # keep letters, numbers, spaces and selected punctuation\n",
    "    pattern = rf'[^\\s{LATIN_BLOCK}{DEVANAGARI_BLOCK}0-9\\?\\!\\.,]'\n",
    "    return regex.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93f67a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_devanagari(text: str) -> str:\n",
    "    if not DEVANAGARI_TO_ROMAN or not isinstance(text, str):\n",
    "        return text\n",
    "    return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db0ee237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_colons(text):\n",
    "    \"\"\"Remove colons around emoji descriptions in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Regex pattern to match emoji descriptions with colons\n",
    "    pattern = r':([a-zA-Z0-9_+-]+):'\n",
    "    # Replace matches with the emoji description without colons\n",
    "    clean_text = re.sub(pattern, r'\\1', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7c3fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ascii_emoticons(text):\n",
    "    \"\"\"Replace ASCII emoticons in text with their descriptions.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    clean_text = text\n",
    "    for emoticon, description in ascii_emoticons.items():\n",
    "        # Escape the emoticon pattern to handle special characters\n",
    "        escaped_emoticon = re.escape(emoticon)\n",
    "        # Build the regex pattern to match emoticons not part of words\n",
    "        pattern = r'(?<!\\w)' + escaped_emoticon + r'(?!\\w)'\n",
    "        # Replace the emoticon with the description, ignoring case\n",
    "        clean_text = re.sub(pattern, f\"{description}\", clean_text, flags=re.IGNORECASE)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "734f129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trade_mark(text):\n",
    "    \"\"\"Replace 'trade_mark' with a single quote (').\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Replace 'trade_mark' with \"'\"\n",
    "    clean_text = text.replace(\"trade_mark\", \"'\")\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbde391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    clean_text = ' '.join(text.split())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cda9dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_functions = [\n",
    "    replace_emojis,\n",
    "    normalize_text,\n",
    "    decode_html_entities,\n",
    "    remove_html_tags,\n",
    "    replace_urls,\n",
    "    replace_emails,\n",
    "    replace_censored_swear_words,\n",
    "    replace_hindi_swears,\n",
    "    replace_mentions,\n",
    "    replace_at_symbol,\n",
    "    remove_hashtags,\n",
    "    expand_contractions,\n",
    "    clean_unicode_punct,\n",
    "    transliterate_devanagari,\n",
    "    remove_emoji_colons,\n",
    "    replace_ascii_emoticons,\n",
    "    replace_trade_mark,\n",
    "    remove_extra_whitespace\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d3995b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # ftfy first (handles mojibake)\n",
    "    text = fix_text(text)\n",
    "    for fn in cleaning_functions:\n",
    "        text = fn(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e6afe",
   "metadata": {},
   "source": [
    "# Data load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eadd2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\University of Illinois Chicago\\Classes\\CS521\\data files\\consolidated_sarcasm_dataset.csv\")\n",
    "df[\"comment\"] = df[\"comment\"].astype(str).apply(clean_tweet)\n",
    "df = df[[\"comment\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee37390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬ª ha ha nain aisa kuch nahin hai\n",
      "¬ª superb again!! line mat kato kutton, ap qatar mein\n",
      "¬ª aab kaun karega aanhoni ko honi ? dhoni retirement cricket\n",
      "¬ª tum logon ka vikas concept badiya hai\n",
      "¬ª gandu ladki. aabhi to tum 18 ki bhi nahi hui thik se !\n"
     ]
    }
   ],
   "source": [
    "for t in df[\"comment\"].sample(5):\n",
    "    print(\"¬ª\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df547cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"non_sarcastic\": 0, \"sarcastic\": 1}\n",
    "\n",
    "df[\"label\"] = (\n",
    "    df[\"label\"]\n",
    "    .astype(str)          # make sure they are strings\n",
    "    .str.strip()          # remove leading/trailing spaces\n",
    "    .str.lower()          # normalise case\n",
    "    .map(label_map)       # convert to 0 / 1\n",
    ")\n",
    "\n",
    "assert df[\"label\"].isin([0, 1]).all(), \"Unexpected label values!\"\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"comment\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.20,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_dataset = [\n",
    "    {\"comment\": t, \"label\": int(l)} for t, l in zip(train_texts.tolist(), train_labels.tolist())\n",
    "]\n",
    "val_dataset = [\n",
    "    {\"comment\": t, \"label\": int(l)} for t, l in zip(val_texts.tolist(), val_labels.tolist())\n",
    "]\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "val_dataset   = Dataset.from_list(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43355f5f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "826af1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/hing-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1765/1765 [00:00<00:00, 32341.12 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 442/442 [00:00<00:00, 27625.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# pick one public model ID\n",
    "model_name = \"l3cube-pune/hing-bert\"      # or \"nirantk/hinglish-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "               model_name,\n",
    "               num_labels=2\n",
    "            ).to(device)\n",
    "\n",
    "# tokenization ‚Äî¬†use the key you actually stored, here it's \"comment\"\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"comment\"], truncation=True, padding=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a3859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "086d3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manan\\anaconda3\\envs\\cs521\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_58308\\721854265.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Disable wandb or hub logging unless needed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12f06426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [444/444 00:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.487102</td>\n",
       "      <td>0.782805</td>\n",
       "      <td>0.766562</td>\n",
       "      <td>0.916981</td>\n",
       "      <td>0.835052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.401800</td>\n",
       "      <td>0.474221</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.782468</td>\n",
       "      <td>0.909434</td>\n",
       "      <td>0.841187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.638674</td>\n",
       "      <td>0.803167</td>\n",
       "      <td>0.792763</td>\n",
       "      <td>0.909434</td>\n",
       "      <td>0.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.753422</td>\n",
       "      <td>0.800905</td>\n",
       "      <td>0.812721</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.839416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=444, training_loss=0.33638698600970945, metrics={'train_runtime': 38.3338, 'train_samples_per_second': 184.172, 'train_steps_per_second': 11.582, 'total_flos': 272104109010000.0, 'train_loss': 0.33638698600970945, 'epoch': 4.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f1064",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a57cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sarcasm_hingbert_model\\\\tokenizer_config.json',\n",
       " './sarcasm_hingbert_model\\\\special_tokens_map.json',\n",
       " './sarcasm_hingbert_model\\\\vocab.txt',\n",
       " './sarcasm_hingbert_model\\\\added_tokens.json',\n",
       " './sarcasm_hingbert_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"./sarcasm_hingbert_model\"\n",
    "\n",
    "# Save model + tokenizer\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9cfa9",
   "metadata": {},
   "source": [
    "# model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0564ac13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"./sarcasm_hingbert_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "model.eval()  # switch to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3caca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\n",
    "    r\"D:\\University of Illinois Chicago\\Classes\\CS521\\cs521\\labeled_batches\\labeled_batch_30.csv\"\n",
    ")\n",
    "\n",
    "df_new.rename(columns={\"Transliterated_Comment\": \"comment\"}, inplace=True)\n",
    "label_map = {\"non_sarcastic\": 0, \"sarcastic\": 1}\n",
    "df_new[\"label\"] = df_new[\"sarcasm_label\"].astype(str).str.strip().str.lower().map(label_map)\n",
    "df_new = df_new.dropna(subset=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94d336ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"comment\"] = df_new[\"comment\"].astype(str).apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d48847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def predict_labels(text_series):\n",
    "    preds_all = []\n",
    "    loader = DataLoader(text_series.tolist(), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_texts in loader:\n",
    "            enc = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            logits = model(**enc).logits\n",
    "            batch_preds = torch.argmax(logits, dim=1)\n",
    "            preds_all.extend(batch_preds.cpu().numpy())\n",
    "    return np.array(preds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ab413",
   "metadata": {},
   "source": [
    "# Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4b8f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8537\n",
      "Precision: 1.0000\n",
      "Recall   : 0.8537\n",
      "F1 Score : 0.9211\n"
     ]
    }
   ],
   "source": [
    "preds = predict_labels(df_new[\"comment\"])\n",
    "true  = df_new[\"label\"].values.astype(int)\n",
    "\n",
    "acc  = accuracy_score(true, preds)\n",
    "prec = precision_score(true, preds)\n",
    "rec  = recall_score(true, preds)\n",
    "f1   = f1_score(true, preds)\n",
    "\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
