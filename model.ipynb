{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f2314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manan\\anaconda3\\envs\\cs521\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b8f1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af457411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1 CUDA: 12.1\n",
      "GPU detected: NVIDIA GeForce RTX 4090\n",
      "CUDA OK? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
    "print(\"GPU detected:\", torch.cuda.get_device_name(0))\n",
    "print(\"CUDA OK?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eadd2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text = fix_text(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s!?.,]\", \"\", text)\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv(r\"D:\\University of Illinois Chicago\\Classes\\CS521\\data files\\consolidated_sarcasm_dataset.csv\")\n",
    "df['comment'] = df['comment'].astype(str).apply(clean_tweet)\n",
    "df = df[['comment', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df547cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"non_sarcastic\": 0, \"sarcastic\": 1}\n",
    "\n",
    "df[\"label\"] = (\n",
    "    df[\"label\"]\n",
    "    .astype(str)          # make sure they are strings\n",
    "    .str.strip()          # remove leading/trailing spaces\n",
    "    .str.lower()          # normalise case\n",
    "    .map(label_map)       # convert to 0 / 1\n",
    ")\n",
    "\n",
    "assert df[\"label\"].isin([0, 1]).all(), \"Unexpected label values!\"\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"comment\"],\n",
    "    df[\"label\"],\n",
    "    test_size=0.20,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_dataset = [\n",
    "    {\"comment\": t, \"label\": int(l)} for t, l in zip(train_texts.tolist(), train_labels.tolist())\n",
    "]\n",
    "val_dataset = [\n",
    "    {\"comment\": t, \"label\": int(l)} for t, l in zip(val_texts.tolist(), val_labels.tolist())\n",
    "]\n",
    "\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "val_dataset   = Dataset.from_list(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826af1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/hing-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1765/1765 [00:00<00:00, 16031.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442/442 [00:00<00:00, 18798.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# pick one public model ID\n",
    "model_name = \"l3cube-pune/hing-bert\"      # or \"nirantk/hinglish-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\n",
    "               model_name,\n",
    "               num_labels=2\n",
    "            ).to(device)\n",
    "\n",
    "# tokenization â€”Â use the key you actually stored, here it's \"comment\"\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"comment\"], truncation=True, padding=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a3859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086d3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manan\\anaconda3\\envs\\cs521\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Manan\\AppData\\Local\\Temp\\ipykernel_48444\\721854265.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Disable wandb or hub logging unless needed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f06426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [444/444 00:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>0.496456</td>\n",
       "      <td>0.751131</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>0.747170</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.791855</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.836299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.667554</td>\n",
       "      <td>0.789593</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.883019</td>\n",
       "      <td>0.834225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.792956</td>\n",
       "      <td>0.785068</td>\n",
       "      <td>0.807971</td>\n",
       "      <td>0.841509</td>\n",
       "      <td>0.824399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=444, training_loss=0.29890357437837234, metrics={'train_runtime': 46.0975, 'train_samples_per_second': 153.154, 'train_steps_per_second': 9.632, 'total_flos': 460762957923600.0, 'train_loss': 0.29890357437837234, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a57cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sarcasm_hingbert_model\\\\tokenizer_config.json',\n",
       " './sarcasm_hingbert_model\\\\special_tokens_map.json',\n",
       " './sarcasm_hingbert_model\\\\vocab.txt',\n",
       " './sarcasm_hingbert_model\\\\added_tokens.json',\n",
       " './sarcasm_hingbert_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"./sarcasm_hingbert_model\"\n",
    "\n",
    "# Save model + tokenizer\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0564ac13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"./sarcasm_hingbert_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n",
    "model.eval()  # switch to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3caca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(r\"D:\\University of Illinois Chicago\\Classes\\CS521\\cs521\\labeled_batches\\labeled_batch_1.csv\")\n",
    "\n",
    "# Clean & map labels\n",
    "label_map = {\"non_sarcastic\": 0, \"sarcastic\": 1}\n",
    "df_new[\"label\"] = df_new[\"sarcasm_label\"].astype(str).str.strip().str.lower().map(label_map)\n",
    "df_new = df_new.dropna(subset=[\"label\"])\n",
    "\n",
    "# Tokenize new comments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Predict function\n",
    "def predict_labels(comments):\n",
    "    inputs = preprocess_texts(comments)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "    return preds.cpu().numpy()\n",
    "\n",
    "# Run prediction\n",
    "#create the new columns name as comment instead of transliterated_comment\n",
    "df_new.rename(columns={\"Transliterated_Comment\": \"comment\"}, inplace=True)\n",
    "preds = predict_labels(df_new[\"comment\"].tolist())\n",
    "true = df_new[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8529\n",
      "Precision: 1.0000\n",
      "Recall   : 0.8529\n",
      "F1 Score : 0.9206\n"
     ]
    }
   ],
   "source": [
    "acc  = accuracy_score(true, preds)\n",
    "prec = precision_score(true, preds)\n",
    "rec  = recall_score(true, preds)\n",
    "f1   = f1_score(true, preds)\n",
    "\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
